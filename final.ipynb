{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPthTlHucwy+lOFl5sJhnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelloBer/Landslide-Detection/blob/version-control/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM7TicpdovbW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Dense, Flatten, Conv2D, MaxPooling2D, Dropout,\n",
        "                                   BatchNormalization, Input, GlobalAveragePooling2D,\n",
        "                                   SeparableConv2D, Add, Activation)\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import datetime\n",
        "from collections import Counter\n",
        "import gc\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'TRAIN_DATA': '/content/drive/My Drive/train_data/',\n",
        "    'TEST_DATA': '/content/drive/My Drive/test_data/',\n",
        "    'TRAIN_CSV': \"/content/drive/My Drive/train_data/Train.csv\",\n",
        "    'TEST_CSV': \"/content/drive/My Drive/test_data/Test.csv\",\n",
        "    'BATCH_SIZE': 16,  # Reduced for better gradient updates\n",
        "    'IMG_SIZE': 256,\n",
        "    'CHANNELS': 12,\n",
        "    'EPOCHS': 100,\n",
        "    'LEARNING_RATE': 0.0001,\n",
        "    'PATIENCE': 15,\n",
        "    'VALIDATION_SPLIT': 0.2\n",
        "}\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data loading and preprocessing with improved normalization\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_and_normalize_image(image_id, folder_path):\n",
        "        \"\"\"Enhanced image loading with better SAR processing\"\"\"\n",
        "        try:\n",
        "            image_path = os.path.join(folder_path, f\"{image_id}.npy\")\n",
        "            if not os.path.exists(image_path):\n",
        "                return None\n",
        "\n",
        "            img = np.load(image_path)\n",
        "\n",
        "            if len(img.shape) != 3 or img.shape[2] != 12:\n",
        "                return None\n",
        "\n",
        "            img_normalized = np.zeros_like(img, dtype=np.float32)\n",
        "\n",
        "            # Optical bands (0-3) - Enhanced normalization\n",
        "            for band in range(4):\n",
        "                band_data = img[:, :, band].astype(np.float32)\n",
        "\n",
        "                # Remove outliers using percentile clipping\n",
        "                p2, p98 = np.percentile(band_data, [2, 98])\n",
        "                band_data = np.clip(band_data, p2, p98)\n",
        "\n",
        "                # Robust normalization\n",
        "                if p98 > p2:\n",
        "                    img_normalized[:, :, band] = (band_data - p2) / (p98 - p2)\n",
        "                else:\n",
        "                    img_normalized[:, :, band] = 0.5\n",
        "\n",
        "            # SAR bands (4-11) - Improved SAR processing\n",
        "            for band in range(4, 12):\n",
        "                sar_data = img[:, :, band].astype(np.float32)\n",
        "\n",
        "                # Convert to dB with proper handling\n",
        "                sar_positive = np.abs(sar_data)\n",
        "                sar_positive = np.maximum(sar_positive, 1e-12)\n",
        "                sar_db = 10 * np.log10(sar_positive + 1e-12)\n",
        "\n",
        "                # Clip extreme values\n",
        "                sar_db = np.clip(sar_db, -50, 10)\n",
        "\n",
        "                # Normalize to [0, 1]\n",
        "                img_normalized[:, :, band] = (sar_db + 50) / 60\n",
        "\n",
        "            return img_normalized\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {image_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "class BalancedDataGenerator(Sequence):\n",
        "    \"\"\"Improved data generator with better class balancing\"\"\"\n",
        "\n",
        "    def __init__(self, image_ids, labels, folder_path, batch_size=16,\n",
        "                 augment=False, shuffle=True, balance_classes=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = np.array(image_ids)\n",
        "        self.labels = np.array(labels)\n",
        "        self.folder_path = folder_path\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.shuffle = shuffle\n",
        "        self.balance_classes = balance_classes\n",
        "\n",
        "        # Validate files\n",
        "        self.valid_indices = self._validate_files()\n",
        "        if len(self.valid_indices) == 0:\n",
        "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
        "\n",
        "        # Create class-balanced indices if needed\n",
        "        if self.balance_classes:\n",
        "            self._create_balanced_indices()\n",
        "        else:\n",
        "            self.balanced_indices = self.valid_indices.copy()\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "        # Enhanced augmentation\n",
        "        if self.augment:\n",
        "            self.augmenter = ImageDataGenerator(\n",
        "                rotation_range=45,\n",
        "                width_shift_range=0.3,\n",
        "                height_shift_range=0.3,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.3,\n",
        "                horizontal_flip=True,\n",
        "                vertical_flip=True,\n",
        "                fill_mode='reflect',\n",
        "                brightness_range=[0.8, 1.2]\n",
        "            )\n",
        "\n",
        "    def _validate_files(self):\n",
        "        \"\"\"Validate file existence and readability\"\"\"\n",
        "        valid_indices = []\n",
        "        for idx, img_id in enumerate(self.image_ids):\n",
        "            file_path = os.path.join(self.folder_path, f\"{img_id}.npy\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    test_img = np.load(file_path)\n",
        "                    if len(test_img.shape) == 3 and test_img.shape[2] == 12:\n",
        "                        valid_indices.append(idx)\n",
        "                except:\n",
        "                    continue\n",
        "        return np.array(valid_indices)\n",
        "\n",
        "    def _create_balanced_indices(self):\n",
        "        \"\"\"Create balanced sampling indices\"\"\"\n",
        "        # Separate indices by class\n",
        "        valid_labels = self.labels[self.valid_indices]\n",
        "        class_0_indices = self.valid_indices[valid_labels == 0]\n",
        "        class_1_indices = self.valid_indices[valid_labels == 1]\n",
        "\n",
        "        # Oversample minority class\n",
        "        min_class_size = min(len(class_0_indices), len(class_1_indices))\n",
        "        max_class_size = max(len(class_0_indices), len(class_1_indices))\n",
        "\n",
        "        # Create balanced dataset by oversampling\n",
        "        if len(class_0_indices) < len(class_1_indices):\n",
        "            # Oversample class 0\n",
        "            oversample_indices = np.random.choice(class_0_indices,\n",
        "                                                len(class_1_indices) - len(class_0_indices))\n",
        "            class_0_indices = np.concatenate([class_0_indices, oversample_indices])\n",
        "        else:\n",
        "            # Oversample class 1\n",
        "            oversample_indices = np.random.choice(class_1_indices,\n",
        "                                                len(class_0_indices) - len(class_1_indices))\n",
        "            class_1_indices = np.concatenate([class_1_indices, oversample_indices])\n",
        "\n",
        "        self.balanced_indices = np.concatenate([class_0_indices, class_1_indices])\n",
        "        print(f\"Balanced dataset: {len(self.balanced_indices)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.balanced_indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.balanced_indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in batch_indices:\n",
        "            img_id = self.image_ids[i]\n",
        "            label = self.labels[i]\n",
        "\n",
        "            img = DataProcessor.load_and_normalize_image(img_id, self.folder_path)\n",
        "            if img is not None:\n",
        "                if self.augment:\n",
        "                    img = self.augmenter.random_transform(img)\n",
        "                batch_images.append(img)\n",
        "                batch_labels.append(label)\n",
        "\n",
        "        # Ensure we have a full batch\n",
        "        while len(batch_images) < len(batch_indices) and len(batch_images) > 0:\n",
        "            rand_idx = np.random.randint(0, len(batch_images))\n",
        "            batch_images.append(batch_images[rand_idx])\n",
        "            batch_labels.append(batch_labels[rand_idx])\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_labels, dtype=np.float32)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.balanced_indices)\n",
        "\n",
        "class ModelBuilder:\n",
        "    \"\"\"Builds improved CNN architecture\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def residual_block(x, filters, kernel_size=3):\n",
        "        \"\"\"Residual block for better gradient flow\"\"\"\n",
        "        shortcut = x\n",
        "\n",
        "        x = SeparableConv2D(filters, kernel_size, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = SeparableConv2D(filters, kernel_size, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Adjust shortcut if needed\n",
        "        if shortcut.shape[-1] != filters:\n",
        "            shortcut = Conv2D(filters, 1, padding='same')(shortcut)\n",
        "            shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "        x = Add()([x, shortcut])\n",
        "        x = Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def build_improved_model(input_shape):\n",
        "        \"\"\"Build improved CNN with residual connections\"\"\"\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        # Initial convolution\n",
        "        x = Conv2D(32, (7, 7), strides=2, padding='same')(inputs)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
        "\n",
        "        # Residual blocks\n",
        "        x = ModelBuilder.residual_block(x, 64)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = ModelBuilder.residual_block(x, 128)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = ModelBuilder.residual_block(x, 256)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = ModelBuilder.residual_block(x, 512)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "        return model\n",
        "\n",
        "# Custom metrics\n",
        "def precision_m(y_true, y_pred):\n",
        "    y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
        "    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
        "    predicted_positives = tf.reduce_sum(y_pred)\n",
        "    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
        "    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
        "    possible_positives = tf.reduce_sum(y_true)\n",
        "    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "# Improved focal loss\n",
        "def focal_loss(alpha=0.7, gamma=2.0):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
        "\n",
        "        # Calculate focal loss\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        focal_weight = y_true * (1 - y_pred) ** gamma + (1 - y_true) * y_pred ** gamma\n",
        "\n",
        "        # Binary crossentropy\n",
        "        bce = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "\n",
        "        return tf.reduce_mean(alpha_factor * focal_weight * bce)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"Starting optimized landslide detection training...\")\n",
        "\n",
        "    # Load data\n",
        "    train_df = pd.read_csv(CONFIG['TRAIN_CSV'])\n",
        "    print(f\"Loaded {len(train_df)} training samples\")\n",
        "    print(f\"Class distribution: {train_df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "    # Stratified split\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        np.arange(len(train_df)),\n",
        "        test_size=CONFIG['VALIDATION_SPLIT'],\n",
        "        random_state=42,\n",
        "        stratify=train_df['label']\n",
        "    )\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = BalancedDataGenerator(\n",
        "        image_ids=train_df['ID'].values[train_idx],\n",
        "        labels=train_df['label'].values[train_idx],\n",
        "        folder_path=CONFIG['TRAIN_DATA'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        augment=True,\n",
        "        balance_classes=True\n",
        "    )\n",
        "\n",
        "    val_gen = BalancedDataGenerator(\n",
        "        image_ids=train_df['ID'].values[val_idx],\n",
        "        labels=train_df['label'].values[val_idx],\n",
        "        folder_path=CONFIG['TRAIN_DATA'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        augment=False,\n",
        "        balance_classes=False\n",
        "    )\n",
        "\n",
        "    print(f\"Training batches: {len(train_gen)}\")\n",
        "    print(f\"Validation batches: {len(val_gen)}\")\n",
        "\n",
        "    # Build model\n",
        "    input_shape = (CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], CONFIG['CHANNELS'])\n",
        "    model = ModelBuilder.build_improved_model(input_shape)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
        "        loss=focal_loss(alpha=0.7, gamma=2.0),\n",
        "        metrics=['accuracy', precision_m, recall_m, f1_m]\n",
        "    )\n",
        "\n",
        "    print(\"Model compiled successfully\")\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(\n",
        "            'best_landslide_model.keras',\n",
        "            monitor='val_f1_m',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_f1_m',\n",
        "            mode='max',\n",
        "            patience=CONFIG['PATIENCE'],\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=7,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Calculate class weights\n",
        "    y_train = train_df['label'].values[train_idx]\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=CONFIG['EPOCHS'],\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    model.load_weights('best_landslide_model.keras')\n",
        "\n",
        "    # Threshold optimization\n",
        "    print(\"\\nOptimizing classification threshold...\")\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "\n",
        "    for i in range(len(val_gen)):\n",
        "        batch_x, batch_y = val_gen[i]\n",
        "        pred_batch = model.predict(batch_x, verbose=0)\n",
        "        val_predictions.extend(pred_batch.flatten())\n",
        "        val_labels.extend(batch_y.flatten())\n",
        "\n",
        "    y_probs = np.array(val_predictions)\n",
        "    y_true = np.array(val_labels)\n",
        "\n",
        "    # Find optimal threshold using F1 score\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "    best_metrics = {}\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_probs > thresh).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = thresh\n",
        "\n",
        "            # Calculate per-class F1 scores\n",
        "            report = classification_report(y_true, y_pred, output_dict=True)\n",
        "            best_metrics = {\n",
        "                'threshold': thresh,\n",
        "                'weighted_f1': f1,\n",
        "                'class_0_f1': report['0']['f1-score'],\n",
        "                'class_1_f1': report['1']['f1-score']\n",
        "            }\n",
        "\n",
        "    print(f\"\\nBest threshold: {best_thresh:.3f}\")\n",
        "    print(f\"Weighted F1: {best_metrics['weighted_f1']:.4f}\")\n",
        "    print(f\"Class 0 F1: {best_metrics['class_0_f1']:.4f}\")\n",
        "    print(f\"Class 1 F1: {best_metrics['class_1_f1']:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    y_pred_final = (y_probs > best_thresh).astype(int)\n",
        "    print(f\"\\nFinal Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred_final, target_names=['No Landslide', 'Landslide']))\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Generate test predictions\n",
        "    generate_test_predictions(model, best_thresh)\n",
        "\n",
        "    print(\"Training completed successfully!\")\n",
        "    return model, best_thresh\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training metrics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    metrics = ['accuracy', 'loss', 'f1_m', 'precision_m']\n",
        "    titles = ['Accuracy', 'Loss', 'F1 Score', 'Precision']\n",
        "\n",
        "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        row, col = i // 2, i % 2\n",
        "\n",
        "        if metric in history.history:\n",
        "            axes[row, col].plot(history.history[metric], label=f'Training {title}')\n",
        "            if f'val_{metric}' in history.history:\n",
        "                axes[row, col].plot(history.history[f'val_{metric}'], label=f'Validation {title}')\n",
        "\n",
        "            axes[row, col].set_title(title)\n",
        "            axes[row, col].set_xlabel('Epoch')\n",
        "            axes[row, col].set_ylabel(title)\n",
        "            axes[row, col].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_test_predictions(model, threshold):\n",
        "    \"\"\"Generate test predictions\"\"\"\n",
        "    test_df = pd.read_csv(CONFIG['TEST_CSV'])\n",
        "    test_ids = test_df['ID'].values\n",
        "\n",
        "    predictions = []\n",
        "    batch_size = CONFIG['BATCH_SIZE']\n",
        "\n",
        "    print(f\"Generating predictions for {len(test_ids)} test images...\")\n",
        "\n",
        "    for i in range(0, len(test_ids), batch_size):\n",
        "        batch_ids = test_ids[i:i+batch_size]\n",
        "        batch_imgs = []\n",
        "\n",
        "        for img_id in batch_ids:\n",
        "            img = DataProcessor.load_and_normalize_image(img_id, CONFIG['TEST_DATA'])\n",
        "            if img is not None:\n",
        "                batch_imgs.append(img)\n",
        "            else:\n",
        "                batch_imgs.append(np.zeros((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], CONFIG['CHANNELS'])))\n",
        "\n",
        "        batch_imgs = np.array(batch_imgs)\n",
        "        probs = model.predict(batch_imgs, verbose=0).flatten()\n",
        "        preds = (probs > threshold).astype(int)\n",
        "        predictions.extend(preds)\n",
        "\n",
        "        if (i // batch_size + 1) % 10 == 0:\n",
        "            print(f\"Processed {min(i+batch_size, len(test_ids))}/{len(test_ids)}\")\n",
        "\n",
        "    # Create submission\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_ids,\n",
        "        'label': np.array(predictions, dtype=int)\n",
        "    })\n",
        "\n",
        "    submission_df.to_csv('optimized_submission.csv', index=False)\n",
        "    print(f\"Submission saved. Prediction distribution: {Counter(predictions)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    # Run main training\n",
        "    model, best_threshold = main()"
      ]
    }
  ]
}